# **El _boom_ de los datos y el protagonismo de la tecnología**



```{block2, type='flushright', html.tag='p'}
_A diferencia de las cosas materiales --los alimentos que comemos,_\
_una vela que arde--, el valor de los datos no disminuye cuando estos se usan;_\
_pueden volver a procesarse una y otra vez. Los datos constituyen lo que los_\
_economistas llaman un bien “no rival”: su uso por una persona no impide_\
_que los use otra. Y la información no se desgasta con el uso, _\
_como sí lo hacen los bienes materiales._\
--- **[@mayer2013big]**
```

Los datos, en especial desde finales del siglo pasado y hasta nuestros días, han dejado de ser un recurso escaso y de bajo interés en el ámbito de la gestión organizacional y se están convirtiendo en uno abundante, necesario y de imprescindible abordaje en el contexto de la gestión moderna. Su auge es tal que hoy, en algunas latitudes, estos han sido elevados a la categoría de activo y recurso estratégico de carácter inagotable, dada su capacidad para ser usados en múltiples propósitos.

El origen del _boom_ que hoy experimentan los datos es diverso, disperso y complejo. Algunos de los responsables de este auge son: las organizaciones privadas y públicas, que se gestionan en un contexto de libre competencia con alcances globales, esto les exige contar con más y mejor información para tomar decisiones que les permitan sobrevivir, pero, sobre todo, crecer; los gobiernos, que han cambiado sus prioridades de proveer los bienes y servicios que las sociedades demandan por regular, evaluar y certificar a través del uso de información la provisión de servicios por parte de terceros y por sus entidades; las sociedades modernas, cada vez más formadas, que demandan tanto del Estado como de sus organizaciones mayores niveles de transparencia, rendiciones permanentes de cuentas y acceso a las cifras derivadas de su gestión; el internet y las redes sociales, que han permitido comunicaciones veloces y la posibilidad de compartir todo tipo de información, incluidos sentimientos expresados en palabras y conversaciones; el crecimiento de las transacciones electrónicas y de las aplicaciones tecnológicas, que proveen nuevos bienes y servicios; el surgimiento de nuevos artefactos, que hacen posible capturar, almacenar, procesar y tomar decisiones autónomas basadas en cientos, miles y millones de datos recolectados; el aumento de manera vertiginosa de la capacidad de almacenamiento, de cómputo, de acceso, y la posibilidad de compartir códigos a nivel mundial, entre otros.

## **Los datos**

Los datos, según [@Ety], citado en el [Conpes 39201](https://colaboracion.dnp.gov.co/CDT/Conpes/Econ%C3%B3micos/3920.pdf), son “la representación primaria de variables cualitativas y cuantitativas que son almacenables, transferibles, pueden ser visualizados, controlados y entendidos”^[Conpes 3920, Política Nacional de Explotación de Datos (_Big Data_), p. 25.]. Así mismo, según este documento, los datos en la actualidad pueden ser clasificados en estructurados, semiestructurados y no estructurados según su grado de afinidad con las siguientes definiciones^[Según este mismo documento, los datos a su vez pueden ser clasificados desde una perspectiva orgánica en públicos y privados, y desde un criterio cualitativo en personales e impersonales.].

+ **Estructurados:** están organizados conforme a un modelo o esquema. Se almacenan en forma tabular y algunas veces su estructura también incluye la definición de las relaciones entre ellos. Típicamente están representados en bases de datos que hacen parte del funcionamiento de sistemas de información.

+ **Semiestructurados:** su organización y presentación tiene una estructura básica (etiquetas o marcadores), pero no tienen establecida una definición de relaciones en su contenido. En esta categoría se incluyen contenidos de correos electrónicos, tuits, archivos XML.

+ **No estructurados:** su organización y presentación no está guiada por ningún modelo o esquema. En esta categoría se incluyen las imágenes, texto, audios, contenidos de redes sociales, videos, entre otros.

Hace no más de 30 años el mundo de los datos estaba gobernado por aquellos de naturaleza estructurada; hoy estos conservan un lugar especial, pero el protagonismo está pasando, gracias a la incidencia de las nuevas tecnologías, al escenario de lo semiestructurado y lo no estructurado. Esta realidad ha implicado grandes transformaciones, entre ellas, en la forma como los datos son capturados, procesados y, sobre todo, analizados. De un escenario del análisis de datos centrado en la matemática y la estadística, estamos incursionando en uno donde están participando nuevos fenómenos, disciplinas y marcos de trabajo soportados en gran medida en las nuevas tecnologías de la información y las comunicaciones. El _Big Data_, la analítica o minería de datos, la inteligencia de negocios y la ciencia de los datos se han sumado hoy a la matemática y la estadística para conformar un verdadero arsenal capaz de enfrentar y extraer el conocimiento oculto en conjuntos de datos que hoy crecen de manera vertiginosa en cantidad y variedad.


## **Big data**

Con la llegada del siglo XXI y con el desarrollo de las nuevas tecnologías de la información y las comunicaciones, hemos superado nuestras capacidades para: almacenar y conservar grandes cantidades de datos, extender el espectro a una gran variedad de datos, y procesar y extraer de manera rápida información contenida en estos para generar nuevo conocimiento o tomar mejores decisiones. A la gran cantidad y variedad de datos que la humanidad está generando a diario, así como la capacidad de almacenamiento y la velocidad con la que estos están siendo procesados se le conoce como _Big Data_^[Por ejemplo, a manera de ilustración de este fenómeno y según \@LoriLewis y \@OfficiallyChadd, actualmente _en un minuto_ en internet se realizan 3,7 millones de búsquedas en Google, se ven 4,3 millones de horas de video en YouTube, se ven 266.000 horas de contenido en Netflix, se envían 481.000 tuits, se intercambian 38 millones de mensajes a través de WhatsApp, se realizan 862.823 compras en línea, se envían 187 millones de correos electrónicos, se inician 973.000 sesiones en Facebook, se descargan 375.000 aplicaciones de App Store y Google Play, etc. Y, actualmente, la humanidad está en capacidad de almacenar y procesar toda esta cantidad de datos para distintos propósitos.]

Aunque no existe precisión sobre el significado de Big Data, este fenómeno, según Mayer-Schönberger y Cukier “se refiere a cosas que se pueden hacer a gran escala, pero no a una escala inferior, para extraer nuevas percepciones o crear nuevas formas de valor, de tal forma que trasforman los mercados, las organizaciones, las relaciones entre los ciudadanos y los gobiernos, etc.” (2013, p. 17]. Así mismo, según estos autores, el término más adecuado para describir lo que en la actualidad está sucediendo a raíz de este fenómeno es la datificación del todo: “Datificar se refiere a recopilar información sobre cuanto existe bajo el sol –incluyendo cosas que en modo alguno solíamos considerar información antes, como la localización de una persona, las vibraciones de un motor o la tensión que soporta un puente– y transformarla a formato de datos para cuantificarlas” (p. 28).

En conclusión, el fenómeno del _Big Data_ se caracteriza por la intención contemporánea de convertir en datos (datificar), dada la capacidad tecnológica actualmente disponible, “una inmensa cantidad de cosas que antes nunca pudieron medirse, almacenarse, analizarse y compartirse” [@mayer2013big, p. 31]. El surgimiento del _Big Data_ como la estrategia que cobija la intención de datificar, se caracteriza principalmente por tres rasgos definitorios popularizados a través de las 3 V: volumen, velocidad y variedad^[Además del volumen, la variedad y la velocidad, al fenómeno del Big Data se le asocian en menor medida otros rasgos característicos como la veracidad de los datos, la viabilidad para extraer conocimiento, el valor potencial de los mismos, así como la capacidad de estos para ser representados de manera gráfica.].

En primer lugar, el volumen hace referencia a la cantidad creciente de datos disponibles (_datificación_) y a los retos que estos están generando tanto para su almacenamiento como para su análisis. Del byte^[En el sitio web https://es.wikipedia.org/wiki/Byte se puede explorar el tamaño de las diferentes unidades actualmente existentes para la medición del almacenamiento de información a nivel tecnológico.] de años atrás pasamos rápidamente a los megabytes; hoy ya no nos son ajenos términos como los gigabytes o terabytes y seguramente, en un futuro no muy lejano, serán de uso cotidiano términos como zettabytes y yottabytes. Estos términos, que hacen referencia a la unidad de medida que representa el tamaño de una tabla/archivo que contiene un conjunto dado de datos, vienen incursionando de manera vertiginosa en nuestra cotidianidad gracias al aumento exponencial de la capacidad tecnológica de almacenamiento disponible, a los bajos costos, a la existencia de múltiples y variados mecanismos de captura de datos, a la facilidad de acceso a los datos por el crecimiento del internet, al surgimiento de fuertes movimientos de apertura de datos y la presencia de nuevas apuestas tecnológicas como la computación en la nube (_cloud computing_).

El segundo rasgo del _Big Data_ es la _variedad_, que hace referencia a los diferentes tipos de datos que son extraídos en la actualidad desde múltiples y diversas fuentes de información. De un mundo analógico, pasando por uno digital, hemos evolucionado hasta uno en donde todo cuanto está a nuestro alrededor es susceptible de ser reducido a datos, es decir, _datificado_. Datos asociados a imágenes, audios, textos, videos, datos georreferenciados, datos provenientes de millones de sitios web, etc., se han sumado al contexto tradicional de información estructurada para conformar una verdadera explosión tanto en el volumen como en la variedad de tipos de datos actualmente disponibles y susceptibles de ser analizados.

La _variedad_ en los tipos de datos hoy existentes exige ir más allá del paradigma tradicional de datos estructurados e incursionar en el mundo de los datos semiestructurados y no estructurados^[En la actualidad se estima que entre un 80 y un 90% de los datos existentes a nivel mundial corresponden a datos semiestructurados y, en especial, a datos no estructurados, lo que contrasta, por ejemplo, con la concentración de una proporción importante de las capacidades académicas, analíticas, y en una menor medida tecnológicas, centradas en la gestión de datos de tipo estructurado.]. La variedad, a diferencia del volumen, sí es una de las grandes responsables del surgimiento del término _Big Data_ pues este fenómeno no existía años atrás y su tratamiento actual ha implicado grandes trasformaciones en la forma como los diversos tipos de datos existentes deben ser capturados, cómo deben ser almacenamos y conservados y, sobre todo, cómo y a través de qué técnicas deben ser analizados.

La _velocidad_ es la tercera característica que define el _Big Data_. Hoy la disponibilidad y oportunidad con la que se entrega la información extraída a partir de los datos juega un rol central en la sostenibilidad y el crecimiento de muchas organizaciones, en especial aquellas de naturaleza privada. Horas, minutos e incluso segundos caracterizan, en muchos casos, aquello que se considera oportuno. En este sentido, los métodos tradicionales de análisis y entrega de información requieren de nuevas y creativas formas de abordar los datos, escenario donde la tecnología juega un rol central. La velocidad con la que se procesa y entrega la información es fundamental para las organizaciones de carácter público y privado, no obstante, hay importantes diferencias entre lo que se considera oportuno al interior de estos contextos. Por ejemplo, en el ámbito de las universidades, la oportunidad en las cifras puede estar asociada con disponer de información cuantitativa de manera anual, semestral o a lo sumo mensual. Desde luego que estas temporalidades en la disposición de cifras del contexto universitario son, por ejemplo, inoportunas en ámbitos como el de los mercados accionarios.

La variedad en los datos hoy disponibles, el volumen y la velocidad con la que estos están siendo generados, así como la capacidad existente para ser analizados ha conllevado una verdadera revolución en la forma como actualmente los capturamos, los almacenamos, los procesamos y extraemos conocimiento a partir de ellos.


A la encuesta y el registro administrativo como mecanismos tradicionales de captura de datos se han adicionado nuevos instrumentos, principalmente tecnológicos, como los sensores, las cámaras, los móviles, los sistemas de posicionamiento global (GPS por sus siglas en inglés), los secuenciadores a gran escala de ADN, los telescopios, las transacciones electrónicas globalizadas, las redes sociales, la web, etc. De la misma manera, el aumento significativo en el volumen y la variedad de datos hoy existentes, sumado a la existencia de redes tecnológicas de comunicación modernas, ha permitido la construcción de bases de datos capaces de hacer uso de recursos tecnológicos compartidos en múltiples servidores ubicados en distintas latitudes de nuestro planeta, y ha superado el paradigma tradicional de las bases de datos SQL, y ha incursionado en el de las bases de datos no solamente SQL o NoSQL^[Aunque no existe precisión sobre la definición de una base de datos NoSQL, estas se caracterizan por su capacidad de réplica y de distribución del almacenamiento de los datos en tiempo real en múltiples servidores.]. Este fenómeno ha implicado evolucionar a nuevas y especializadas formas de capturar, almacenar y acceder a datos no estructurados o semiestructurados, principalmente. A los proveedores tradicionales de bases de datos SQL como Oracle, MySQL, PosgreSQL, Microsoft SQL Server, ODBC, SQLite, DB2, etc., hoy se suman nuevos proveedores especializados como Cassandra, mongoDB, Neo4j, Apache HBASE, Redis, CouchDB, GoogleBigtable, los cuales han llegado para quedarse y para consolidar un amplio, diverso y creciente escenario de bases de datos y, con ellos, de nuevas y variadas fuentes de datos disponibles para el análisis y la extracción de la información allí contenida.

El gran volumen de datos existente, su variedad y la velocidad con la que se desea extraer conocimiento a partir de los mismos ha implicado a nivel tecnológico la implementación de innovadoras formas de procesamiento de los datos dada la baja capacidad de los métodos tradicionales estadísticos para abordar estos retos en los tiempos requeridos y a unos costos computaciones aceptables. Algunas de las nuevas formas de analizar la información en un mundo gobernado por el _Big Data_ son los modelos de computación paralela que hacen uso de paradigmas computacionales como MapReduce, en donde se divide un gran problema de análisis de datos en cientos, miles o millones de pequeños problemas en igual cantidad de nodos o servidores que trabajan de manera simultánea en lugares diferentes y bajo marcos de trabajo _frameworks_ como Hadoop o Spark.

El crecimiento en la cantidad de datos, en su variedad y en la velocidad con la que se debe generar información a partir de los mismos, así como la reciente formulación en el contexto público colombiano, como se presentó en el capítulo anterior, de una política nacional de explotación de datos Big Data (Conpes 3920), presiona en la actualidad a las entidades, y entre estas a las universidades, para que, además de una gestión y disposición de las cifras descriptivas institucionales provistas por marcos tradicionales, incursionen en el dominio y uso de técnicas que permiten extraer conocimiento que se encuentra oculto en las cifras y que no es adquirible a partir de una aproximación descriptiva de los datos institucionales disponibles.

## **Analítica/minería de datos (_analytics_)**

La expresión _analítica de datos_, al igual que el _Big Data_, se posiciona durante los primeros años de este siglo, a pesar de que muchas de las técnicas que hoy soportan esta disciplina se venían estudiando y desarrollando desde los años setenta del siglo pasado bajo el rótulo de la llamada minería de datos^[Aunque no existe claridad acerca del porqué del uso actual del término analytics o analítica como sustituto del término minería de datos, se cree que este último se ha usado con la intención de modernizar el mismo y adaptarlo al contexto actual del mercado de los datos. Desde luego que el término analytics es más comercial y despierta mayor interés que el término minería de datos. Para propósitos del presente documento, cuando hagamos referencia a los términos analítica de datos, minería de datos o analytics, nos referimos a lo mismo.]. En el contexto de las organizaciones, y entre estas en las entidades públicas, el uso de la analítica o minería de datos tiene un propósito central: la toma de decisiones institucionales a partir de la extracción de conocimiento oculto existente en datos estructurados, semiestructurados y no estructurados, y expresable a través de patrones extrapolables a escenarios futuros (enfoque predictivo).

La extracción de patrones existente en pequeños, grandes o diversos conjuntos de datos es el objetivo central de la analítica de datos (_analytics_), y para ello se apoya en dos recursos principales: los algoritmos y la disposición de cientos de técnicas o métodos que hacen uso de estos para múltiples propósitos. Los _algoritmos_ son el método de facto empleado por las técnicas de analítica de datos y los mismos pueden ser entendidos como un conjunto de instrucciones o reglas claramente definidas, las cuales se ejecutan mediante pasos sucesivos con el fin de encontrar un estado final deseado. Los pasos sucesivos requeridos en la ejecución de un algoritmo pueden, en muchos casos, alcanzar cifras de cientos o millones por lo que estos deben valerse de las capacidades de cómputo actualmente existentes para su viabilidad.

Aunque en la actualidad, en muchos casos, los algoritmos se consideran como creaciones ajenas que poco impactan nuestra cotidianidad, su uso está cada vez más presente en nuestro día a día. Recomendaciones de compras en plataformas de internet, autocompletado de palabras en aplicaciones de mensajería instantánea, autocompletado de consultas en la web, acceso a recursos financieros en cuestión de minutos, acceso a dispositivos a través de reconocimiento facial, publicidad personalizada de acuerdo al consumo y los gustos expresados en la web y en redes sociales, recomendaciones musicales y de televisión gracias a nuestros consumos o el de personas con gustos o características semejantes, toma de decisiones autónomas por parte de las cosas que están a nuestro alrededor (internet de las cosas), etc., son tan solo una pequeña muestra de lo cercano que hoy están los algoritmos a nuestra cotidianidad, del uso de los datos hoy existentes y, con ello del surgimiento de un mundo transformado digitalmente y gobernado por estos instrumentos cuyos impactos políticos, sociales, económicos e incluso familiares están aún por verse.

Una pequeña muestra de los métodos y las técnicas que hoy conforman el cuerpo de la analítica o minería de datos son: la minería de texto, el análisis de redes, la minería de imágenes, la minería de audios, el análisis de asociación, los árboles de decisión, las redes neuronales, los métodos de clasificación, la regresión logística, la regresión lineal, el aprendizaje de máquina, los vectores de soporte de máquinas, el aprendizaje profundo, la inteligencia artificial, la analítica de procesos, etc. Estos métodos usan cientos, miles y millones de algoritmos para su buen desempeño, viven entre nosotros, son ampliamente empleados por entidades privadas y, en especial, por aquellas de base tecnológica y, como reza el Conpes 3920, se busca que sean empleados cada vez más dentro de la cultura de las organizaciones públicas colombianas, tanto para la toma de decisiones institucionales informadas como para la prestación de mejores servicios de cara a los ciudadanos.

La analítica o minería de datos (_analytics_) –al igual que el fenómeno del _Big Data_– puede ser usada para múltiples propósitos y para ello cuenta con un cuerpo de recursos metodológicos y tecnológicos propios. En primer lugar, esta puede ser usada para propósitos descriptivos, explicativos o predictivos y para ello se vale de dos enfoques centrales: el análisis supervisado y el no supervisado. El _análisis no supervisado_ usa técnicas de minería de datos descriptiva o exploratoria para obtener patrones o perfiles donde no es de interés la disposición de variables dependientes sobre las cuales se desee describir o predecir un comportamiento dado; las técnicas de clasificación, por ejemplo, hacen parte del análisis no supervisado. En contraste, el _análisis supervisado_ hace uso de técnicas para comprender y predecir el comportamiento de un evento futuro con base en eventos que ya pasaron y sobre los cuales se cuenta con información precisa –variables independientes, variables dependientes, conjuntos de datos de prueba y conjuntos de datos de entrenamiento–; las regresiones, los árboles de decisión, las redes neuronales, etc., por ejemplo, hacen parte del análisis supervisado desde la perspectiva de la analítica o minería de datos.

En segundo lugar, la analítica de datos requiere de la existencia, el dominio y el uso de metodologías y rutinas soportadas en _software_ especializados capaces de ejecutar múltiples algoritmos asociados a las técnicas existentes. Entre los software más populares hoy disponibles para la ejecución de técnicas de minería de datos se encuentran, en el ámbito comercial, KMINE, RapidMiner, SAS e IBM y, en el escenario del _software_ libre, R y Python principalmente.

## **Inteligencia de negocios**